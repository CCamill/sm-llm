# ä½¿ç”¨ Qwen-Coder ç”Ÿæˆä»£ç  Embedding æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

ä½ å¯ä»¥ä½¿ç”¨ Qwen2.5-Coder-7B æ¥ç”Ÿæˆä»£ç çš„ embedding å‘é‡ï¼Œä½†éœ€è¦äº†è§£ä¸€äº›æŠ€æœ¯ç»†èŠ‚ã€‚

## ğŸ”§ æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨é‡åŒ–çš„ Qwen-Coderï¼ˆå·²æä¾›è„šæœ¬ï¼‰

### æ˜¾å­˜éœ€æ±‚ä¼°ç®—

| é‡åŒ–æ–¹å¼ | æ˜¾å­˜å ç”¨ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|
| 4-bit (NF4) | ~5-6 GB | æ¨èï¼Œ4090 å®Œå…¨å¤Ÿç”¨ |
| 8-bit | ~8-10 GB | ç²¾åº¦æ›´é«˜ï¼Œ4090 è¶³å¤Ÿ |
| æ— é‡åŒ– (bf16) | ~14 GB | æœ€ä½³ç²¾åº¦ï¼Œ4090 å¯ä»¥ä½†ä½™é‡å° |

### ä½¿ç”¨æ–¹æ³•

```bash
# å®‰è£…ä¾èµ–
pip install torch transformers accelerate bitsandbytes sentencepiece

# è¿è¡Œè„šæœ¬
python qwen_coder_embedding.py
```

### å…³é”®æŠ€æœ¯ç‚¹

1. **Pooling ç­–ç•¥**
   - `last`: å–æœ€åä¸€ä¸ª token çš„ hidden stateï¼ˆæœ€å¸¸ç”¨ï¼‰
   - `mean`: æ‰€æœ‰ token çš„å¹³å‡å€¼
   - å¯¹äº decoder-only æ¨¡å‹ï¼Œ`last` é€šå¸¸æ•ˆæœæœ€å¥½

2. **L2 å½’ä¸€åŒ–**
   - å½’ä¸€åŒ–åè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æ›´æ–¹ä¾¿
   - ç›´æ¥ä½¿ç”¨ç‚¹ç§¯å³å¯

---

## âš¡ æ–¹æ¡ˆäºŒï¼šæ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆæ¨èï¼‰

å¦‚æœä½ çš„ä¸»è¦ç›®æ ‡æ˜¯**ä»£ç è¯­ä¹‰æœç´¢æˆ–ç›¸ä¼¼åº¦è®¡ç®—**ï¼Œæœ‰ä¸“é—¨ä¸ºæ­¤è®¾è®¡çš„ embedding æ¨¡å‹ï¼Œæ•ˆæœæ›´å¥½ä¸”æ›´è½»é‡ï¼š

### æ¨èæ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|-----|------|-----|
| `jinaai/jina-embeddings-v3` | ~0.5B | ä»£ç /æ–‡æœ¬é€šç”¨ï¼ŒMTEB é«˜åˆ† |
| `BAAI/bge-code-embedding-v1.5` | ~0.3B | ä¸“ä¸ºä»£ç è®¾è®¡ |
| `microsoft/codebert-base` | ~125M | å¾®è½¯çš„ä»£ç  BERT |
| `Salesforce/codet5p-110m-embedding` | ~110M | CodeT5+ çš„ embedding ç‰ˆæœ¬ |

### ç¤ºä¾‹ï¼šä½¿ç”¨ sentence-transformers

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½ä¸“é—¨çš„ä»£ç  embedding æ¨¡å‹ï¼ˆæ›´å°æ›´å¿«ï¼‰
model = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True)

# è·å– embedding
codes = [
    "def quick_sort(arr): ...",
    "function quickSort(arr) { ... }",
]
embeddings = model.encode(codes)

# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity([embeddings[0]], [embeddings[1]])
```

---

## ğŸ¤” ä»€ä¹ˆæ—¶å€™ç”¨ Qwen-Coderï¼Ÿ

### é€‚åˆä½¿ç”¨ Qwen-Coder ç”Ÿæˆ embedding çš„åœºæ™¯ï¼š

1. **éœ€è¦ç†è§£ä»£ç ä¸Šä¸‹æ–‡å’Œæ„å›¾**
   - å¦‚åˆ¤æ–­ä»£ç åŠŸèƒ½ã€bug æ¨¡å¼ç­‰
   
2. **éœ€è¦ç»“åˆç”Ÿæˆèƒ½åŠ›**
   - å¦‚å…ˆç†è§£ä»£ç å†ç”Ÿæˆå»ºè®®
   
3. **å·²ç»åœ¨ç”¨ Qwen-Coder åšå…¶ä»–ä»»åŠ¡**
   - å¤ç”¨æ¨¡å‹ï¼Œå‡å°‘èµ„æº

### ä¸å¤ªé€‚åˆçš„åœºæ™¯ï¼š

1. **çº¯ç²¹çš„ä»£ç æ£€ç´¢/æœç´¢**
   - ä¸“é—¨çš„ embedding æ¨¡å‹æ•ˆæœæ›´å¥½
   
2. **å¤§è§„æ¨¡æ‰¹é‡å¤„ç†**
   - 7B æ¨¡å‹å¤ªå¤§ï¼Œä¸“ç”¨æ¨¡å‹æ›´é«˜æ•ˆ
   
3. **éœ€è¦ä½å»¶è¿Ÿ**
   - å¤§æ¨¡å‹æ¨ç†æ…¢

---

## ğŸš€ è¿›é˜¶ï¼šä½¿ç”¨ LLM2Vec æ–¹æ³•

å¦‚æœæƒ³è®© decoder-only æ¨¡å‹è¡¨ç°å¾—æ›´åƒ encoderï¼ˆæ›´å¥½çš„ embeddingï¼‰ï¼Œå¯ä»¥è€ƒè™‘ LLM2Vec æ–¹æ³•ï¼š

```python
# å®‰è£…
pip install llm2vec

from llm2vec import LLM2Vec

# è¿™ä¼šä¿®æ”¹æ¨¡å‹ä½¿å…¶æ”¯æŒåŒå‘æ³¨æ„åŠ›
model = LLM2Vec.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B",
    peft_model_name_or_path="...",  # éœ€è¦å¯¹åº”çš„ LoRA æƒé‡
    device_map="cuda",
    torch_dtype=torch.bfloat16,
)
```

æ³¨æ„ï¼šLLM2Vec éœ€è¦ç‰¹å®šçš„ LoRA æƒé‡ï¼Œå¯èƒ½æ²¡æœ‰ç°æˆçš„ Qwen-Coder ç‰ˆæœ¬ã€‚

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”å‚è€ƒ

| æ–¹æ³• | embedding è´¨é‡ | é€Ÿåº¦ | æ˜¾å­˜ |
|-----|--------------|------|------|
| Qwen-Coder-7B (4bit) | â­â­â­ | â­â­ | 5-6 GB |
| ä¸“ç”¨ code embedding æ¨¡å‹ | â­â­â­â­ | â­â­â­â­â­ | <2 GB |
| LLM2Vec æ”¹é€  | â­â­â­â­â­ | â­â­ | 5-6 GB |

---

## æ€»ç»“

1. **å¦‚æœä½ æƒ³å°è¯•ç”¨ Qwen-Coder**ï¼šä½¿ç”¨æä¾›çš„ `qwen_coder_embedding.py` è„šæœ¬
2. **å¦‚æœä½ è¿½æ±‚æœ€ä½³ embedding æ•ˆæœ**ï¼šä½¿ç”¨ä¸“é—¨çš„ä»£ç  embedding æ¨¡å‹
3. **4090 æ˜¾å­˜å®Œå…¨è¶³å¤Ÿ**è¿è¡Œ 4-bit é‡åŒ–çš„ 7B æ¨¡å‹
