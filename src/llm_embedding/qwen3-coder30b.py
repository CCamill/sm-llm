"""
ä½¿ç”¨é‡åŒ–åŠ è½½çš„ Qwen3-Coder-30B ç”Ÿæˆä»£ç  Embedding
é€‚ç”¨äº RTX 4090 (24GB VRAM)

æ¨¡å‹é€‰æ‹©è¯´æ˜ï¼š
- Qwen/Qwen3-32B: Qwen3 åŸºç¡€æ¨¡å‹ 32B
- Qwen/Qwen3-30B-A3B: Qwen3 MoE æ¨¡å‹ï¼ˆ30B æ€»å‚æ•°ï¼Œ3B æ¿€æ´»å‚æ•°ï¼‰
- Qwen/Qwen2.5-Coder-32B-Instruct: Qwen2.5 ä»£ç ä¸“ç”¨æ¨¡å‹ 32B

å®‰è£…ä¾èµ–ï¼š
pip install torch transformers accelerate bitsandbytes sentencepiece --break-system-packages

å¯¹äº 30B+ æ¨¡å‹ï¼Œ4-bit é‡åŒ–åçº¦éœ€ 15-18GB æ˜¾å­˜
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from typing import List, Union, Optional, Literal
import numpy as np
import gc


class Qwen3CoderEmbedding:
    """
    ä½¿ç”¨ Qwen3-Coder æˆ–å…¶ä»–å¤§å‹ Qwen æ¨¡å‹ç”Ÿæˆä»£ç  embedding
    æ”¯æŒ 4-bit é‡åŒ– + CPU Offload ä»¥é€‚åº”æœ‰é™æ˜¾å­˜
    """
    
    # å¸¸ç”¨æ¨¡å‹åˆ—è¡¨
    AVAILABLE_MODELS = {
        # Qwen3 ç³»åˆ—
        "qwen3-32b": "Qwen/Qwen3-32B",
        "qwen3-32b-instruct": "Qwen/Qwen3-32B-Instruct",
        "qwen3-30b-a3b": "Qwen/Qwen3-30B-A3B",  # MoE æ¨¡å‹
        "qwen3-30b-a3b-instruct": "Qwen/Qwen3-30B-A3B-Instruct",
        
        # Qwen2.5-Coder ç³»åˆ—ï¼ˆä»£ç ä¸“ç”¨ï¼‰
        "qwen2.5-coder-32b": "Qwen/Qwen2.5-Coder-32B",
        "qwen2.5-coder-32b-instruct": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "qwen2.5-coder-14b-instruct": "Qwen/Qwen2.5-Coder-14B-Instruct",
        "qwen2.5-coder-7b-instruct": "Qwen/Qwen2.5-Coder-7B-Instruct",
    }
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-Coder-32B-Instruct",
        quantization: Literal["4bit", "8bit", "none"] = "4bit",
        use_flash_attention: bool = True,
        max_memory: Optional[dict] = None,
        cpu_offload: bool = False,
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯å®Œæ•´è·¯å¾„æˆ–ç®€ç§°ï¼ˆè§ AVAILABLE_MODELSï¼‰
            quantization: é‡åŒ–æ–¹å¼
            use_flash_attention: æ˜¯å¦ä½¿ç”¨ Flash Attention 2ï¼ˆéœ€è¦å®‰è£… flash-attnï¼‰
            max_memory: æ˜¾å­˜é™åˆ¶ï¼Œå¦‚ {"cuda:0": "20GB", "cpu": "30GB"}
            cpu_offload: æ˜¯å¦å¯ç”¨ CPU offloadï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰
        """
        # è§£ææ¨¡å‹åç§°
        if model_name.lower() in self.AVAILABLE_MODELS:
            model_name = self.AVAILABLE_MODELS[model_name.lower()]
        
        self.model_name = model_name
        print(f"="*60)
        print(f"åˆå§‹åŒ–æ¨¡å‹: {model_name}")
        print(f"="*60)
        
        # é…ç½®é‡åŒ–
        quantization_config = self._get_quantization_config(quantization)
        
        # é…ç½® attention
        attn_implementation = "flash_attention_2" if use_flash_attention else "sdpa"
        
        # åŠ è½½ tokenizer
        print("åŠ è½½ Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left",  # å¯¹äº decoder-only æ¨¡å‹ï¼Œå·¦å¡«å……æ›´å¥½
        )
        
        # ç¡®ä¿æœ‰ pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # é…ç½® device_map
        if cpu_offload:
            # è‡ªåŠ¨åˆ†é…ï¼Œå…è®¸ CPU offload
            device_map = "auto"
            if max_memory is None:
                max_memory = {
                    "cuda": "22GB",  # ä¸º 4090 ç•™ä¸€äº›ä½™é‡
                    "cpu": "48GB",
                }
            print(f"å¯ç”¨ CPU Offloadï¼Œå†…å­˜é™åˆ¶: {max_memory}")
        else:
            device_map = "auto"
        
        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹ï¼ˆ{quantization} é‡åŒ–ï¼‰...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map=device_map,
                max_memory=max_memory,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                attn_implementation=attn_implementation,
                low_cpu_mem_usage=True,
            )
        except Exception as e:
            print(f"Flash Attention åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ° SDPA: {e}")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map=device_map,
                max_memory=max_memory,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
                attn_implementation="sdpa",
                low_cpu_mem_usage=True,
            )
        
        self.model.eval()
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        self.hidden_size = self.model.config.hidden_size
        self._print_model_info()
    
    def _get_quantization_config(self, quantization: str) -> Optional[BitsAndBytesConfig]:
        """è·å–é‡åŒ–é…ç½®"""
        if quantization == "4bit":
            print("âœ“ ä½¿ç”¨ 4-bit NF4 é‡åŒ– + åŒé‡é‡åŒ–")
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
            )
        elif quantization == "8bit":
            print("âœ“ ä½¿ç”¨ 8-bit é‡åŒ–")
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
            )
        else:
            print("âœ“ ä¸ä½¿ç”¨é‡åŒ–ï¼ˆéœ€è¦å¤§é‡æ˜¾å­˜ï¼‰")
            return None
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"  Embedding ç»´åº¦: {self.hidden_size}")
        print(f"  æ¨¡å‹å±‚æ•°: {self.model.config.num_hidden_layers}")
        
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"  GPU æ˜¾å­˜: {allocated:.2f}GB / {total:.2f}GB")
        
        # æ£€æŸ¥æ¨¡å‹åˆ†å¸ƒ
        if hasattr(self.model, 'hf_device_map'):
            devices = set(self.model.hf_device_map.values())
            print(f"  æ¨¡å‹åˆ†å¸ƒ: {devices}")
        print(f"{'='*60}\n")
    
    def get_embedding(
        self,
        texts: Union[str, List[str]],
        pooling: Literal["last", "mean", "weighted_mean"] = "last",
        normalize: bool = True,
        max_length: int = 4096,
        batch_size: int = 1,
        layer: int = -1,  # -1 è¡¨ç¤ºæœ€åä¸€å±‚
        show_progress: bool = True,
    ) -> np.ndarray:
        """
        è·å–æ–‡æœ¬çš„ embedding å‘é‡
        
        Args:
            texts: è¾“å…¥æ–‡æœ¬
            pooling: æ± åŒ–æ–¹å¼
                - "last": æœ€åä¸€ä¸ª tokenï¼ˆæ¨èï¼‰
                - "mean": å¹³å‡æ± åŒ–
                - "weighted_mean": ä½ç½®åŠ æƒå¹³å‡ï¼ˆåé¢çš„ token æƒé‡æ›´é«˜ï¼‰
            normalize: L2 å½’ä¸€åŒ–
            max_length: æœ€å¤§ token é•¿åº¦
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆå¤§æ¨¡å‹å»ºè®®è®¾ä¸º 1ï¼‰
            layer: ä½¿ç”¨ç¬¬å‡ å±‚çš„ hidden stateï¼Œ-1 è¡¨ç¤ºæœ€åä¸€å±‚
            show_progress: æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            embedding å‘é‡ï¼Œshape: (batch_size, hidden_size)
        """
        if isinstance(texts, str):
            texts = [texts]
        
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            if show_progress and len(texts) > batch_size:
                print(f"  å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")
            
            # Tokenize
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
            )
            
            # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.model(
                    **inputs,
                    output_hidden_states=True,
                    return_dict=True,
                )
            
            # è·å–æŒ‡å®šå±‚çš„ hidden states
            hidden_states = outputs.hidden_states[layer]
            attention_mask = inputs["attention_mask"]
            
            # æ± åŒ–
            embeddings = self._pool(hidden_states, attention_mask, pooling)
            
            # å½’ä¸€åŒ–
            if normalize:
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.cpu().numpy())
            
            # æ¸…ç†æ˜¾å­˜
            del outputs, hidden_states
            torch.cuda.empty_cache()
        
        return np.concatenate(all_embeddings, axis=0)
    
    def _pool(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        pooling: str,
    ) -> torch.Tensor:
        """æ± åŒ–æ“ä½œ"""
        if pooling == "last":
            # è·å–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªæœ‰æ•ˆ token
            seq_lengths = attention_mask.sum(dim=1) - 1
            batch_size = hidden_states.size(0)
            embeddings = hidden_states[
                torch.arange(batch_size, device=hidden_states.device),
                seq_lengths
            ]
        
        elif pooling == "mean":
            # å¹³å‡æ± åŒ–ï¼ˆæ’é™¤ paddingï¼‰
            mask = attention_mask.unsqueeze(-1).float()
            embeddings = (hidden_states * mask).sum(dim=1) / mask.sum(dim=1)
        
        elif pooling == "weighted_mean":
            # ä½ç½®åŠ æƒå¹³å‡ï¼ˆåé¢çš„ token æƒé‡æ›´é«˜ï¼‰
            batch_size, seq_len, _ = hidden_states.shape
            weights = torch.arange(1, seq_len + 1, device=hidden_states.device).float()
            weights = weights.unsqueeze(0).expand(batch_size, -1)
            weights = weights * attention_mask.float()
            weights = weights / weights.sum(dim=1, keepdim=True)
            embeddings = (hidden_states * weights.unsqueeze(-1)).sum(dim=1)
        
        else:
            raise ValueError(f"Unknown pooling: {pooling}")
        
        return embeddings
    
    def compute_similarity(
        self,
        text1: str,
        text2: str,
        **kwargs,
    ) -> float:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        emb1 = self.get_embedding(text1, show_progress=False, **kwargs)
        emb2 = self.get_embedding(text2, show_progress=False, **kwargs)
        return float(np.dot(emb1[0], emb2[0]))
    
    def find_most_similar(
        self,
        query: str,
        candidates: List[str],
        top_k: int = 5,
        **kwargs,
    ) -> List[tuple]:
        """åœ¨å€™é€‰åˆ—è¡¨ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æœ¬"""
        query_emb = self.get_embedding(query, show_progress=False, **kwargs)
        candidate_embs = self.get_embedding(candidates, **kwargs)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(candidate_embs, query_emb[0])
        
        # æ’åº
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        return [(candidates[i], float(similarities[i])) for i in top_indices]


def estimate_memory(model_params_b: float, quantization: str) -> float:
    """ä¼°ç®—æ˜¾å­˜éœ€æ±‚ï¼ˆGBï¼‰"""
    if quantization == "4bit":
        return model_params_b * 0.5 + 2  # 4-bit â‰ˆ 0.5 bytes/param + overhead
    elif quantization == "8bit":
        return model_params_b * 1.0 + 2
    else:
        return model_params_b * 2.0 + 2  # bf16 = 2 bytes/param


def main():
    print("\n" + "="*60)
    print("Qwen3/Qwen2.5-Coder å¤§æ¨¡å‹ Embedding ç¤ºä¾‹")
    print("="*60)
    
    # æ˜¾å­˜ä¼°ç®—
    print("\nğŸ“Š æ˜¾å­˜éœ€æ±‚ä¼°ç®— (4-bit é‡åŒ–):")
    print(f"  Qwen3-32B:           ~{estimate_memory(32, '4bit'):.1f} GB")
    print(f"  Qwen3-30B-A3B (MoE): ~{estimate_memory(30, '4bit'):.1f} GB (å®é™…æ›´ä½)")
    print(f"  Qwen2.5-Coder-32B:   ~{estimate_memory(32, '4bit'):.1f} GB")
    print(f"  RTX 4090 æ˜¾å­˜:       24 GB")
    
    # é€‰æ‹©æ¨¡å‹
    # å¯¹äº 4090ï¼Œæ¨èä»¥ä¸‹é€‰é¡¹ï¼š
    # 1. "qwen2.5-coder-32b-instruct" - ä»£ç ä¸“ç”¨ï¼Œæ•ˆæœæœ€å¥½
    # 2. "qwen3-30b-a3b-instruct" - MoE æ¨¡å‹ï¼Œå®é™…æ¿€æ´»å‚æ•°å°‘
    # 3. "qwen3-32b-instruct" - é€šç”¨æ¨¡å‹
    
    print("\nğŸš€ åŠ è½½æ¨¡å‹...")
    
    # ä½¿ç”¨ Qwen2.5-Coder-32Bï¼ˆä»£ç ä¸“ç”¨ï¼Œæ¨èï¼‰
    # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œä¼šè‡ªåŠ¨ offload åˆ° CPU
    embedder = Qwen3CoderEmbedding(
        model_name="Qwen/Qwen3-Coder-30B-A3B-Instruct",  # æˆ– "qwen3-32b-instruct"
        quantization="4bit",
        use_flash_attention=True,
        cpu_offload=True,  # æ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨
        max_memory={
            "cuda:0": "22GB",  # ä¸º 4090 ç•™ä½™é‡
            "cpu": "64GB",     # æ ¹æ®ä½ çš„å†…å­˜è°ƒæ•´
        },
    )
    
    # æµ‹è¯•ä»£ç ç¤ºä¾‹
    code_samples = [
        # 1. Python å¿«é€Ÿæ’åº
        '''def quick_sort(arr):
    """Quick sort implementation"""
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)''',
        
        # 2. Python å½’å¹¶æ’åº
        '''def merge_sort(arr):
    """Merge sort implementation"""
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)''',
        
        # 3. Python HTTP è¯·æ±‚
        '''import requests

def fetch_data(url):
    """Fetch data from URL"""
    response = requests.get(url)
    response.raise_for_status()
    return response.json()''',
        
        # 4. JavaScript å¿«é€Ÿæ’åº
        '''function quickSort(arr) {
    if (arr.length <= 1) return arr;
    const pivot = arr[Math.floor(arr.length / 2)];
    const left = arr.filter(x => x < pivot);
    const middle = arr.filter(x => x === pivot);
    const right = arr.filter(x => x > pivot);
    return [...quickSort(left), ...middle, ...quickSort(right)];
}''',
        
        # 5. Rust å¿«é€Ÿæ’åº
        '''fn quick_sort<T: Ord + Clone>(arr: &[T]) -> Vec<T> {
    if arr.len() <= 1 {
        return arr.to_vec();
    }
    let pivot = arr[arr.len() / 2].clone();
    let left: Vec<_> = arr.iter().filter(|&x| x < &pivot).cloned().collect();
    let middle: Vec<_> = arr.iter().filter(|&x| x == &pivot).cloned().collect();
    let right: Vec<_> = arr.iter().filter(|&x| x > &pivot).cloned().collect();
    [quick_sort(&left), middle, quick_sort(&right)].concat()
}''',
    ]
    
    labels = [
        "Python QuickSort",
        "Python MergeSort", 
        "Python HTTP",
        "JS QuickSort",
        "Rust QuickSort",
    ]
    
    # ç”Ÿæˆ embeddings
    print("\nğŸ“ ç”Ÿæˆä»£ç  Embeddings...")
    embeddings = embedder.get_embedding(
        code_samples,
        pooling="last",
        batch_size=1,  # å¤§æ¨¡å‹ç”¨å° batch
    )
    print(f"Embedding shape: {embeddings.shape}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print("\n" + "="*60)
    print("ä»£ç ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰:")
    print("="*60)
    
    # è¡¨å¤´
    print(f"\n{'':18s}", end="")
    for label in labels:
        print(f"{label:15s}", end="")
    print()
    print("-" * (18 + 15 * len(labels)))
    
    # ç›¸ä¼¼åº¦çŸ©é˜µ
    for i, label_i in enumerate(labels):
        print(f"{label_i:18s}", end="")
        for j in range(len(labels)):
            sim = float(np.dot(embeddings[i], embeddings[j]))
            print(f"{sim:15.3f}", end="")
        print()
    
    # åˆ†æ
    print("\n" + "="*60)
    print("ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ:")
    print("="*60)
    
    pairs = [
        (0, 3, "Python vs JS QuickSort", "ç›¸åŒç®—æ³•ï¼Œä¸åŒè¯­è¨€"),
        (0, 4, "Python vs Rust QuickSort", "ç›¸åŒç®—æ³•ï¼Œä¸åŒè¯­è¨€"),
        (0, 1, "QuickSort vs MergeSort", "ä¸åŒæ’åºç®—æ³•"),
        (0, 2, "QuickSort vs HTTP", "å®Œå…¨ä¸åŒåŠŸèƒ½"),
        (3, 4, "JS vs Rust QuickSort", "ç›¸åŒç®—æ³•ï¼Œä¸åŒè¯­è¨€"),
    ]
    
    for i, j, name, desc in pairs:
        sim = float(np.dot(embeddings[i], embeddings[j]))
        print(f"  {name}: {sim:.4f}")
        print(f"    â†’ {desc}")
    
    # è¯­ä¹‰æœç´¢ç¤ºä¾‹
    print("\n" + "="*60)
    print("ğŸ” è¯­ä¹‰æœç´¢ç¤ºä¾‹:")
    print("="*60)
    
    query = "sorting algorithm implementation"
    print(f"\næŸ¥è¯¢: '{query}'")
    print("\næœ€ç›¸ä¼¼çš„ä»£ç ç‰‡æ®µ:")
    
    results = embedder.find_most_similar(query, code_samples, top_k=3)
    for i, (code, sim) in enumerate(results, 1):
        preview = code[:50].replace('\n', ' ') + "..."
        print(f"  {i}. [{sim:.4f}] {preview}")
    
    print("\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()